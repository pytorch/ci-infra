SHELL=/bin/bash -o pipefail
PROHOME = ../../..

REGION = $(notdir $(CURDIR))
ACCOUNT = $(notdir $(patsubst %/,%,$(dir $(CURDIR))))

export RAW_GHA_PRIVATE_KEY = $(shell aws secretsmanager get-secret-value --secret-id github/ci-infra/app-private-key --query SecretString | jq -r | jq '."private-key"' | jq -r)
export GHA_PRIVATE_KEY = $(shell echo -e $RAW_GHA_PRIVATE_KEY)
export AWS_PROFILE = $(ACCOUNT)

.PHONY: all
all: k8s-runner-scaler 

.PHONY: venv
venv: $(PROHOME)/venv/bin/pip

$(PROHOME)/venv/bin/pip:
	cd $(PROHOME)/ && virtualenv venv
	$(PROHOME)/venv/bin/pip install -r $(PROHOME)/requirements.txt

$(PROHOME)/tf-modules/VERSIONS: $(PROHOME)/venv/bin/pip $(PROHOME)/Terrafile
	$(PROHOME)/venv/bin/python $(PROHOME)/scripts/terrafile_lambdas.py -t $(PROHOME)/Terrafile -m $(PROHOME)/tf-modules

.PHONY: terrafile
terrafile: $(PROHOME)/tf-modules/VERSIONS

.terraform/modules/modules.json: $(PROHOME)/tf-modules/VERSIONS backend.tf
	terraform init

.PHONY: init
init: .terraform/modules/modules.json

.PHONY: clean
clean:
	$(RM) -r .terraform
	$(RM) .terraform.lock.hcl
	$(RM) backend-state.tf
	$(RM) backend.plan
	$(RM) backend.tf
	$(RM) dyn_locals.tf
	$(RM) external_k8s_cidr_ipv4.tf
	$(RM) terraform.tfstate
	cd $(PROHOME)/ && make clean

.PHONY: backend-state
backend-state: backend.tf

external_k8s_cidr_ipv4.tf: .account-checked $(PROHOME)/venv/bin/pip $(PROHOME)/scripts/simplify_cidr_blocks.py
	$(PROHOME)/venv/bin/python $(PROHOME)/scripts/simplify_cidr_blocks.py --rules-per-sg 50 --output-file external_k8s_cidr_ipv4.tf

dyn_locals.tf: .account-checked
	echo -e "locals {\n  aws_region = \"$(REGION)\"\n  aws_account_id = \"$(ACCOUNT)\"\n}\n" >dyn_locals.tf

backend.tf: backend-state.tf
	sed "s/#AWS_REGION/$(REGION)/g" <$(PROHOME)/modules/backend-file/backend.tf >backend.tf
	$(RM) terraform.tfstate

backend-state.tf: .account-checked dyn_locals.tf external_k8s_cidr_ipv4.tf
	sed "s/#AWS_REGION/$(REGION)/g" <$(PROHOME)/modules/backend-file/backend-state.tf >backend-state.tf
	terraform get -update
	terraform init -backend=false
	terraform plan -out=backend.plan -target=module.backend-state
	terraform apply backend.plan

.account-checked:
	aws configure list-profiles | grep $(ACCOUNT) || (echo "Account $(ACCOUNT) not configured in ~/.aws/config", see https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html; exit 1)
	echo OK >.account-checked

.PHONY: plan
plan: .terraform/modules/modules.json
	terraform plan

.PHONY: apply
apply: .terraform/modules/modules.json
	terraform apply

.PHONY: destroy
destroy: .terraform/modules/modules.json
	terraform destroy

.PHONY: update-kubectl
update-kubectl: apply
	aws eks update-kubeconfig --region $(REGION) --name ghci-runners-c7g-4xl
	aws eks update-kubeconfig --region $(REGION) --name ghci-runners-g4dn-4xl

.PHONY: apply-cert-manager
apply-cert-manager: update-kubectl
	kubectl config use-context arn:aws:eks:$(REGION):$(ACCOUNT):cluster/ghci-runners-c7g-4xl
	kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.yaml
	kubectl config use-context arn:aws:eks:$(REGION):$(ACCOUNT):cluster/ghci-runners-g4dn-4xl
	kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.yaml

.PHONY: add-helm-repository
add-helm-repository:
	helm repo add actions-runner-controller https://actions-runner-controller.github.io/actions-runner-controller

.PHONY: install-arc
install-arc: apply-cert-manager add-helm-repository
	[ "$$GHA_PRIVATE_KEY" != "" ] || (echo "GHA_PRIVATE_KEY not set"; exit 1)
	kubectl config use-context arn:aws:eks:$(REGION):$(ACCOUNT):cluster/ghci-runners-c7g-4xl
	helm upgrade --install --namespace actions-runner-system --create-namespace --set=replicaCount=2 --set=authSecret.create=true --set=authSecret.github_app_id="343735" --set=authSecret.github_app_installation_id="38323217" --set=authSecret.github_app_private_key="$$GHA_PRIVATE_KEY" --wait actions-runner-controller actions-runner-controller/actions-runner-controller
	kubectl config use-context arn:aws:eks:$(REGION):$(ACCOUNT):cluster/ghci-runners-g4dn-4xl
	helm upgrade --install --namespace actions-runner-system --create-namespace --set=replicaCount=2 --set=authSecret.create=true --set=authSecret.github_app_id="343735" --set=authSecret.github_app_installation_id="38323217" --set=authSecret.github_app_private_key="$$GHA_PRIVATE_KEY" --wait actions-runner-controller actions-runner-controller/actions-runner-controller

.PHONY: k8s-runner-scaler
k8s-runner-scaler: install-arc
	kubectl config use-context arn:aws:eks:$(REGION):$(ACCOUNT):cluster/ghci-runners-c7g-4xl
	kubectl apply -f $(PROHOME)/rundeployment/storageclass.yaml --namespace=actions-runner-system
	kubectl apply -f $(PROHOME)/rundeployment/claim-medium.yaml --namespace=actions-runner-system
	kubectl apply -f $(PROHOME)/rundeployment/claim-large.yaml --namespace=actions-runner-system
	NAME='linux.c7g.4xlarge' envsubst < $(PROHOME)/rundeployment/runnerdeployment.yaml | kubectl apply --namespace=actions-runner-system -f-
	NAME='linux.c7g.4xlarge' envsubst < $(PROHOME)/rundeployment/horizontalautoscaler.yaml | kubectl apply --namespace=actions-runner-system -f-
	kubectl config use-context arn:aws:eks:$(REGION):$(ACCOUNT):cluster/ghci-runners-g4dn-4xl
	kubectl apply -f $(PROHOME)/rundeployment/storageclass.yaml --namespace=actions-runner-system
	kubectl apply -f $(PROHOME)/rundeployment/claim-medium.yaml --namespace=actions-runner-system
	kubectl apply -f $(PROHOME)/rundeployment/claim-large.yaml --namespace=actions-runner-system
	NAME='linux.g4dn.4xlarge' envsubst < $(PROHOME)/rundeployment/runnerdeployment.yaml | kubectl apply --namespace=actions-runner-system -f-
	NAME='linux.g4dn.4xlarge' envsubst < $(PROHOME)/rundeployment/horizontalautoscaler.yaml | kubectl apply --namespace=actions-runner-system -f-


.PHONY: delete
delete:
	kubectl delete -f $(PROHOME)/rundeployment/storageclass.yaml --namespace=actions-runner-system --force || true
	kubectl delete -f $(PROHOME)/rundeployment/claim-medium.yaml --namespace=actions-runner-system --force || true
	kubectl delete -f $(PROHOME)/rundeployment/claim-large.yaml --namespace=actions-runner-system --force || true
	kubectl delete -f $(PROHOME)/rundeployment/runnerdeployment.yaml --namespace=actions-runner-system --force || true
	kubectl delete -f $(PROHOME)/rundeployment/horizontalautoscaler.yaml --namespace=actions-runner-system --force || true

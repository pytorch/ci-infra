SHELL=/bin/bash -o pipefail
PROHOME = $(realpath ../../..)

REGION = $(notdir $(CURDIR))
ACCOUNT = $(notdir $(patsubst %/,%,$(dir $(CURDIR))))

export AWS_PROFILE = $(ACCOUNT)

.PHONY: all
all: k8s-runner-scaler

.PHONY: venv
venv: $(PROHOME)/venv/bin/pip

$(PROHOME)/venv/bin/pip:
	cd $(PROHOME)/ && virtualenv venv
	$(PROHOME)/venv/bin/pip install -r $(PROHOME)/requirements.txt

$(PROHOME)/tf-modules/VERSIONS: $(PROHOME)/venv/bin/pip $(PROHOME)/Terrafile
	$(PROHOME)/venv/bin/python $(PROHOME)/scripts/terrafile_lambdas.py -t $(PROHOME)/Terrafile -m $(PROHOME)/tf-modules

.PHONY: terrafile
terrafile: $(PROHOME)/tf-modules/VERSIONS

.terraform/modules/modules.json: $(PROHOME)/tf-modules/VERSIONS backend.tf
	terraform init

.PHONY: init
init: .terraform/modules/modules.json

.PHONY: clean
clean:
	$(RM) -r .terraform
	$(RM) .terraform.lock.hcl
	$(RM) backend-state.tf
	$(RM) backend.plan
	$(RM) backend.tf
	$(RM) dyn_locals.tf
	$(RM) external_k8s_cidr_ipv4.tf
	$(RM) terraform.tfstate
	cd $(PROHOME)/ && make clean

.PHONY: backend-state
backend-state: backend.tf

external_k8s_cidr_ipv4.tf: .account-checked $(PROHOME)/venv/bin/pip $(PROHOME)/scripts/simplify_cidr_blocks.py
	$(PROHOME)/venv/bin/python $(PROHOME)/scripts/simplify_cidr_blocks.py --rules-per-sg 50 --output-file external_k8s_cidr_ipv4.tf

dyn_locals.tf: .account-checked
	echo -e "locals {\n  aws_region = \"$(REGION)\"\n  aws_account_id = \"$(ACCOUNT)\"\n}\n" >dyn_locals.tf

backend.tf: backend-state.tf
	sed "s/#AWS_REGION/$(REGION)/g" <$(PROHOME)/modules/backend-file/backend.tf >backend.tf
	$(RM) terraform.tfstate

backend-state.tf: .account-checked dyn_locals.tf external_k8s_cidr_ipv4.tf
	sed "s/#AWS_REGION/$(REGION)/g" <$(PROHOME)/modules/backend-file/backend-state.tf >backend-state.tf
	terraform get -update
	terraform init -backend=false
	terraform plan -input=false -out=backend.plan -detailed-exitcode -target=module.backend-state ${TERRAFORM_EXTRAS} ; \
		ext_code=$$? ; \
		if [ $$ext_code -eq 2 ] ; then \
			terraform apply backend.plan ${TERRAFORM_EXTRAS} ; \
		elif [ $$ext_code -eq 0 ] ; then \
			echo "Backend state already exists" ; \
		else \
			echo "Unexpected exit code $$ext_code" ; \
			exit 1 ; \
		fi

.account-checked:
	aws configure list-profiles | grep $(ACCOUNT) || (echo "Account $(ACCOUNT) not configured in ~/.aws/config", see https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html; exit 1)
	echo OK >.account-checked

.PHONY: plan
plan: .terraform/modules/modules.json
	terraform plan $(TERRAFORM_EXTRAS)

.PHONY: apply
apply: .terraform/modules/modules.json
	terraform apply ${TERRAFORM_EXTRAS}

.PHONY: apply-arc-canary
apply-arc-canary: .terraform/modules/modules.json
	terraform apply --target=module.arc_canary ${TERRAFORM_EXTRAS}

.PHONY: apply-arc-vanguard
apply-arc-vanguard: .terraform/modules/modules.json
	terraform apply --target=module.arc_vanguard ${TERRAFORM_EXTRAS}

.PHONY: apply-arc-prod
apply-arc-prod: .terraform/modules/modules.json
	terraform apply --target=module.arc_prod ${TERRAFORM_EXTRAS}

.PHONY: destroy
destroy: .terraform/modules/modules.json
	echo "To make sure you want to run this, go to the Makefile and comment out the destroy target" ; exit 1
	# terraform destroy ${TERRAFORM_EXTRAS}

.PHONY: tflint
tflint: .terraform/modules/modules.json
	tflint --init
	tflint --module --color --minimum-failure-severity=warning --recursive


##################### INVENTORY ###########################
inventory/eks/canary_cluster_name: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json canary_eks_cluster_name | jq -r '.[] | [.] | @csv' | sed 's/"\(.*\)"/\1/' >inventory/eks/canary_cluster_name

inventory/eks/canary_cluster_config: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json canary_eks_config | jq . >inventory/eks/canary_cluster_config

inventory/eks/vanguard_cluster_name: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json vanguard_eks_cluster_name | jq -r '.[] | [.] | @csv' | sed 's/"\(.*\)"/\1/' >inventory/eks/vanguard_cluster_name

inventory/eks/vanguard_cluster_config: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json vanguard_eks_config | jq . >inventory/eks/vanguard_cluster_config

inventory/eks/prod_cluster_name: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json prod_eks_cluster_name | jq -r '.[] | [.] | @csv' | sed 's/"\(.*\)"/\1/' >inventory/eks/prod_cluster_name

inventory/eks/prod_cluster_config: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json prod_eks_config | jq . >inventory/eks/prod_cluster_config


####################### ARC #############################
ARC_NODE_CONFIG.yaml:
	if [ ! -z "$${GITHUB_TOKEN}" ] ; then \
		curl -o $@ -H "Authorization: Bearer $${GITHUB_TOKEN}" https://raw.githubusercontent.com/pytorch/test-infra/main/.github/arc-node-config.yaml ; \
	else \
		curl -o $@ https://raw.githubusercontent.com/pytorch/test-infra/main/.github/arc-node-config.yaml ; \
	fi

ARC_RUNNER_CONFIG.yaml:
	if [ ! -z "$${GITHUB_TOKEN}" ] ; then \
		curl -o $@ -H "Authorization: Bearer $${GITHUB_TOKEN}" https://raw.githubusercontent.com/pytorch/test-infra/main/.github/arc-runner-config.yaml ; \
	else \
		curl -o $@ https://raw.githubusercontent.com/pytorch/test-infra/main/.github/arc-runner-config.yaml ; \
	fi

# Canary
.PHONY: arc-canary
arc-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=808166 \
				GHA_INST_ID=46578864 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY_CANARY \
				EKS_ENVIRONMENT=canary \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				clean-k8s-rds-state install-arc setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds || exit 1 ; \
		done

.PHONY: karpenter-autoscaler-canary
karpenter-autoscaler-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state setup-karpenter-autoscaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				EKS_ENVIRONMENT=canary \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				clean-k8s-rds-state install-arc setup-karpenter-autoscaler delete-stale-rds || exit 1 ; \
		done

.PHONY: k8s-runner-scaler-canary
k8s-runner-scaler-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state k8s-runner-scaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=808166 \
				GHA_INST_ID=46578864 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY_CANARY \
				EKS_ENVIRONMENT=canary \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				clean-k8s-rds-state install-arc k8s-runner-scaler delete-stale-rds  || exit 1 ; \
		done

# Vanguard
.PHONY: arc-vanguard
arc-vanguard: inventory/eks/vanguard_cluster_name inventory/eks/vanguard_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=343735 \
				GHA_INST_ID=38323217 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY \
				EKS_ENVIRONMENT=vanguard \
				PROJECTTAG=gi-ci-vanguard \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-org \
				MINRUNNERS=30 \
				MAXRUNNERS=30 \
				clean-k8s-rds-state install-arc setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds || exit 1 ; \
		done

.PHONY: arc-vanguard-off
arc-vanguard-off: inventory/eks/vanguard_cluster_name inventory/eks/vanguard_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=343735 \
				GHA_INST_ID=38323217 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY \
				EKS_ENVIRONMENT=vanguard \
				PROJECTTAG=gi-ci-vanguard \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-org \
				MINRUNNERS=0 \
				MAXRUNNERS=0 \
				clean-k8s-rds-state install-arc setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds || exit 1 ; \
		done

# Prod
.PHONY: arc-prod
arc-prod: inventory/eks/prod_cluster_name inventory/eks/prod_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/prod_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=343735 \
				GHA_INST_ID=38323217 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY \
				EKS_ENVIRONMENT=prod \
				PROJECTTAG=gi-ci-prod \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/prod_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-org \
				clean-k8s-rds-state install-arc setup-karpenter-autoscaler k8s-runner-scaler delete-stale-rds || exit 1 ; \
		done

.PHONY: eks-use-cluster
eks-use-cluster:
	cd $(PROHOME)/modules/arc ; $(MAKE) EKS_CLUSTER_NAME=$(CLUSTER) update-kubectl

# Deployment
.PHONY: open-rel-pr
open-rel-pr: $(PROHOME)/venv/bin/pip
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug open-rel-pr

.PHONY: wait-check-deployment
wait-check-deployment: $(PROHOME)/venv/bin/pip
	[ "$(RELEASE_ACTION_NAME)" != "" ] || (echo "RELEASE_ACTION_NAME not set"; exit 1)
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug wait-check-deployment --release-action-name "$(RELEASE_ACTION_NAME)" --comment-to-add "$(COMMENT_TO_ADD)" --ignore-if-label "$(IGNORE_IF_LABEL)"

.PHONY: wait-check-user-comment
wait-check-user-comment: $(PROHOME)/venv/bin/pip
	[ "$(WAIT_COMMENT)" != "" ] || (echo "WAIT_COMMENT not set"; exit 1)
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug wait-check-user-comment --comment "$(WAIT_COMMENT)"

.PHONY: wait-check-bot-comment
wait-check-bot-comment: $(PROHOME)/venv/bin/pip
	[ "$(WAIT_COMMENT)" != "" ] || (echo "WAIT_COMMENT not set"; exit 1)
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug wait-check-bot-comment --comment "$(WAIT_COMMENT)"

.PHONY: react-pr-comment
react-pr-comment: $(PROHOME)/venv/bin/pip
	[ "$(COMMENTS)" != "" ] || (echo "COMMENTS not set"; exit 1)
	[ "$(LABELS)" != "" ] || (echo "LABELS not set"; exit 1)
	[ "$(CHECK_REMOVE_LABELS)" != "" ] || (echo "CHECK_REMOVE_LABELS not set"; exit 1)
	[ "$(CHECK_COMMENTS)" != "" ] || (echo "CHECK_COMMENTS not set"; exit 1)
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug react-pr-comment --comments "$(COMMENTS)" --labels "$(LABELS)" --check-remove-labels "$(CHECK_REMOVE_LABELS)" --check-comments "$(CHECK_COMMENTS)"

.PHONY: add-comment-to-pr
add-comment-to-pr: $(PROHOME)/venv/bin/pip
	[ "$(COMMENT_TO_ADD)" != "" ] || (echo "COMMENT_TO_ADD not set"; exit 1)
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug add-comment-to-pr --comment "$(COMMENT_TO_ADD)"

.PHONY: wait-check-pr-approved
wait-check-pr-approved: $(PROHOME)/venv/bin/pip
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug wait-check-pr-approved

.PHONY: close-pr
close-pr: $(PROHOME)/venv/bin/pip
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug close-pr

.PHONY: merge-pr
merge-pr: $(PROHOME)/venv/bin/pip
	$(PROHOME)/venv/bin/python ./scripts/deployment.py --debug merge-pr


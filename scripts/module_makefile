SHELL=/bin/bash -o pipefail
PROHOME = $(realpath ../../..)

REGION = $(notdir $(CURDIR))
ACCOUNT = $(notdir $(patsubst %/,%,$(dir $(CURDIR))))

ifneq ($(GITHUB_ACTIONS),true)
export AWS_PROFILE = $(ACCOUNT)
endif

.PHONY: all
all: k8s-runner-scaler

.PHONY: venv
venv: $(PROHOME)/venv/bin/pip

$(PROHOME)/venv/bin/pip:
	cd $(PROHOME)/ && make venv/bin/pip

$(PROHOME)/tf-modules/VERSIONS: $(PROHOME)/venv/bin/pip $(PROHOME)/Terrafile
	cd $(PROHOME)/ && make tf-modules/VERSIONS

.PHONY: terrafile
terrafile: $(PROHOME)/tf-modules/VERSIONS

.terraform/modules/modules.json: $(PROHOME)/tf-modules/VERSIONS backend.tf ../eks_users
	terraform init

../eks_users:
	if [ -z "$${EKS_USERS}" ] ; then \
		echo "EKS_USERS is not set" ; exit 1 ; \
	fi
	echo "$${EKS_USERS}" | base64 -d | sed 's/ACCOUNT_ID/$(ACCOUNT)/g' >../eks_users

.PHONY: init
init: .terraform/modules/modules.json

.PHONY: clean
clean:
	$(RM) -rf .terraform
	$(RM) -rf inventory
	$(RM) .terraform.lock.hcl
	$(RM) ARC_NODE_CONFIG.yaml
	$(RM) ARC_RUNNER_CONFIG.yaml
	$(RM) backend-state.tf
	$(RM) backend.plan
	$(RM) backend.tf
	$(RM) dyn_locals.tf
	$(RM) external_k8s_cidr_ipv4.tf
	$(RM) terraform.tfstate
	cd $(PROHOME)/modules/arc && make clean
	cd $(PROHOME)/ && make clean

.PHONY: backend-state
backend-state: backend.tf

external_k8s_cidr_ipv4.tf: $(PROHOME)/venv/bin/pip $(PROHOME)/scripts/simplify_cidr_blocks.py
	$(PROHOME)/venv/bin/python $(PROHOME)/scripts/simplify_cidr_blocks.py --rules-per-sg 50 --output-file external_k8s_cidr_ipv4.tf

dyn_locals.tf:
	echo -e "locals {\n  aws_region = \"$(REGION)\"\n  aws_account_id = \"$(ACCOUNT)\"\n}\n" >dyn_locals.tf

backend.tf: backend-state.tf
	sed "s/#AWS_REGION/$(REGION)/g" <$(PROHOME)/modules/backend-file/backend.tf >backend.tf
	$(RM) terraform.tfstate

backend-state.tf: dyn_locals.tf external_k8s_cidr_ipv4.tf
	sed "s/#AWS_REGION/$(REGION)/g" <$(PROHOME)/modules/backend-file/backend-state.tf >backend-state.tf
	terraform get -update
	terraform init -backend=false
	terraform plan -input=false -out=backend.plan -detailed-exitcode -target=module.backend-state -lock-timeout=15m ; \
		ext_code=$$? ; \
		if [ $$ext_code -eq 2 ] ; then \
			if [ "$$GITHUB_ACTIONS" != "true" ] ; then \
				echo "Backend state does not exist, creating" ; \
				terraform apply ${TERRAFORM_EXTRAS} backend.plan ; \
			else \
				echo "Backend state does not exist, should not do it on a github action!" ; \
				exit 1 ; \
			fi ; \
		elif [ $$ext_code -eq 0 ] ; then \
			echo "Backend state already exists" ; \
		else \
			echo "Unexpected exit code $$ext_code" ; \
			exit 1 ; \
		fi

.PHONY: plan
plan: .terraform/modules/modules.json
	terraform plan $(TERRAFORM_EXTRAS)

.PHONY: apply
apply: .terraform/modules/modules.json
	terraform apply ${TERRAFORM_EXTRAS}

.PHONY: apply-arc-canary
apply-arc-canary: .terraform/modules/modules.json
	terraform apply --target=module.arc_canary ${TERRAFORM_EXTRAS}

.PHONY: apply-arc-vanguard
apply-arc-vanguard: .terraform/modules/modules.json
	terraform apply --target=module.arc_vanguard ${TERRAFORM_EXTRAS}

.PHONY: apply-arc-prod
apply-arc-prod: .terraform/modules/modules.json
	terraform apply --target=module.arc_prod ${TERRAFORM_EXTRAS}

.PHONY: destroy
destroy: .terraform/modules/modules.json
	echo "To make sure you want to run this, go to the Makefile and comment out the destroy target" ; exit 1
	# terraform destroy ${TERRAFORM_EXTRAS}

.PHONY: tflint
tflint: .terraform/modules/modules.json
	tflint --init
	tflint --module --color --minimum-failure-severity=warning --recursive


##################### INVENTORY ###########################
inventory/eks/canary_cluster_name: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json canary_eks_cluster_name | jq -r '.[] | [.] | @csv' | sed 's/"\(.*\)"/\1/' >inventory/eks/canary_cluster_name

inventory/eks/canary_cluster_config: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json canary_eks_config | jq . >inventory/eks/canary_cluster_config

inventory/eks/vanguard_cluster_name: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json vanguard_eks_cluster_name | jq -r '.[] | [.] | @csv' | sed 's/"\(.*\)"/\1/' >inventory/eks/vanguard_cluster_name

inventory/eks/vanguard_cluster_config: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json vanguard_eks_config | jq . >inventory/eks/vanguard_cluster_config

inventory/eks/prod_cluster_name: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json prod_eks_cluster_name | jq -r '.[] | [.] | @csv' | sed 's/"\(.*\)"/\1/' >inventory/eks/prod_cluster_name

inventory/eks/prod_cluster_config: .terraform/modules/modules.json
	mkdir -p inventory/eks
	terraform output -json prod_eks_config | jq . >inventory/eks/prod_cluster_config


####################### ARC #############################
ARC_NODE_CONFIG.yaml:
	if [ ! -z "$${GITHUB_TOKEN}" ] ; then \
		curl -o $@ -H "Authorization: Bearer $${GITHUB_TOKEN}" https://raw.githubusercontent.com/pytorch/test-infra/main/.github/arc-node-config.yaml ; \
	else \
		curl -o $@ https://raw.githubusercontent.com/pytorch/test-infra/main/.github/arc-node-config.yaml ; \
	fi

ARC_RUNNER_CONFIG.yaml:
	if [ ! -z "$${GITHUB_TOKEN}" ] ; then \
		curl -o $@ -H "Authorization: Bearer $${GITHUB_TOKEN}" https://raw.githubusercontent.com/pytorch/test-infra/DanilBaibak/Cleanup-Runner-Config/.github/arc-runner-config.yaml ; \
	else \
		curl -o $@ https://raw.githubusercontent.com/pytorch/test-infra/DanilBaibak/Cleanup-Runner-Config/.github/arc-runner-config.yaml ; \
	fi

# Canary
.PHONY: arc-canary
arc-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			if [ "$$CLUSTER_TARGET" != "" ] && [ "$$p" != "$$CLUSTER_TARGET" ] ; then \
				echo "Skipping $$p - as CLUSTER_TARGET is defined as '$$CLUSTER_TARGET'" ; continue ; \
			fi ; \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=808166 \
				GHA_INST_ID=46578864 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY_CANARY \
				EKS_ENVIRONMENT=canary \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				REGION=$(REGION) \
				clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources || exit 1 ; \
		done

.PHONY: install-docker-registry-canary
install-docker-registry-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			if [ "$$CLUSTER_TARGET" != "" ] && [ "$$p" != "$$CLUSTER_TARGET" ] ; then \
				echo "Skipping $$p - as CLUSTER_TARGET is defined as '$$CLUSTER_TARGET'" ; continue ; \
			fi ; \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: install-docker-registry" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=808166 \
				GHA_INST_ID=46578864 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY_CANARY \
				EKS_ENVIRONMENT=canary \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				REGION=$(REGION) \
				install-docker-registry || exit 1 ; \
		done

.PHONY: karpenter-autoscaler-canary
karpenter-autoscaler-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			if [ "$$CLUSTER_TARGET" != "" ] && [ "$$p" != "$$CLUSTER_TARGET" ] ; then \
				echo "Skipping $$p - as CLUSTER_TARGET is defined as '$$CLUSTER_TARGET'" ; continue ; \
			fi ; \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state setup-karpenter-autoscaler delete-stale-rds" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=808166 \
				GHA_INST_ID=46578864 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY_CANARY \
				EKS_ENVIRONMENT=canary \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				REGION=$(REGION) \
				clean-k8s-rds-state setup-karpenter-autoscaler delete-stale-rds || exit 1 ; \
		done

.PHONY: k8s-runner-scaler-canary
k8s-runner-scaler-canary: inventory/eks/canary_cluster_name inventory/eks/canary_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_name | \
		while read p ; do \
			if [ "$$CLUSTER_TARGET" != "" ] && [ "$$p" != "$$CLUSTER_TARGET" ] ; then \
				echo "Skipping $$p - as CLUSTER_TARGET is defined as '$$CLUSTER_TARGET'" ; continue ; \
			fi ; \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state k8s-runner-scaler delete-stale-helm-pkgs" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=808166 \
				GHA_INST_ID=46578864 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY_CANARY \
				EKS_ENVIRONMENT=canary \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				PROJECTTAG=gi-ci-canary \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/canary_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-canary \
				REGION=$(REGION) \
				clean-k8s-rds-state k8s-runner-scaler delete-stale-helm-pkgs || exit 1 ; \
		done

# Vanguard
.PHONY: arc-vanguard
arc-vanguard: inventory/eks/vanguard_cluster_name inventory/eks/vanguard_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_name | \
		while read p ; do \
			if [ "$$CLUSTER_TARGET" != "" ] && [ "$$p" != "$$CLUSTER_TARGET" ] ; then \
				echo "Skipping $$p - as CLUSTER_TARGET is defined as '$$CLUSTER_TARGET'" ; continue ; \
			fi ; \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=343735 \
				GHA_INST_ID=38323217 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY \
				EKS_ENVIRONMENT=vanguard \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				PROJECTTAG=gi-ci-vanguard \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-org \
				MINRUNNERS=30 \
				MAXRUNNERS=30 \
				REGION=$(REGION) \
				clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources || exit 1 ; \
		done

.PHONY: arc-vanguard-off
arc-vanguard-off: inventory/eks/vanguard_cluster_name inventory/eks/vanguard_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_name | \
		while read p ; do \
			if [ "$$CLUSTER_TARGET" != "" ] && [ "$$p" != "$$CLUSTER_TARGET" ] ; then \
				echo "Skipping $$p - as CLUSTER_TARGET is defined as '$$CLUSTER_TARGET'" ; continue ; \
			fi ; \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/vanguard_cluster_config \
				EKS_ENVIRONMENT=vanguard \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				GHA_ID=343735 \
				GHA_INST_ID=38323217 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY \
				MAXRUNNERS=0 \
				MINRUNNERS=0 \
				PROJECTTAG=gi-ci-vanguard \
				RUNNERSCOPE=pytorch-org \
				REGION=$(REGION) \
				clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources || exit 1 ; \
		done

# Prod
.PHONY: arc-prod
arc-prod: inventory/eks/prod_cluster_name inventory/eks/prod_cluster_config $(PROHOME)/venv/bin/pip ARC_NODE_CONFIG.yaml ARC_RUNNER_CONFIG.yaml
	cd $(PROHOME)/modules/arc ; \
		cat $(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/prod_cluster_name | \
		while read p ; do \
			echo "==== Cluster $$p ============================================" ; \
			echo "OPS: clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources" ; \
			$(MAKE) EKS_CLUSTER_NAME=$$p \
				GHA_ID=343735 \
				GHA_INST_ID=38323217 \
				GHA_PRIVATE_KEY_VAR=GHA_PRIVATE_KEY \
				EKS_ENVIRONMENT=prod \
				EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users \
				PROJECTTAG=gi-ci-prod \
				CLUSTER_CONFIG_FILE=$(PROHOME)/aws/$(ACCOUNT)/$(REGION)/inventory/eks/prod_cluster_config \
				ARC_CFG_FILE_FOLDER=$(PROHOME)/aws/$(ACCOUNT)/$(REGION) \
				RUNNERSCOPE=pytorch-org \
				REGION=$(REGION) \
				clean-k8s-rds-state install-arc install-karpenter install-docker-registry setup-karpenter-autoscaler k8s-runner-scaler delete-stale-resources || exit 1 ; \
		done

.PHONY: eks-use-cluster
eks-use-cluster:
	cd $(PROHOME)/modules/arc ; $(MAKE) EKS_USERS_PATH=$(PROHOME)/aws/$(ACCOUNT)/eks_users EKS_CLUSTER_NAME=$(CLUSTER) update-kubectl
